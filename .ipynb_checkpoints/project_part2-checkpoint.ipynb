{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# NLP Project 2: Comparative Analysis of Summarization Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this segment of the research project, we delve into the realm of automated text summarization, focusing on evaluating and comparing the performance of various state-of-the-art language models. The central objective is to understand how different models behave under varying input conditions and to analyze the nuances in their summarization capabilities.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Dataset Generation\n",
    "\n",
    "To facilitate a comprehensive and unbiased comparison, a specialized program has been developed. This program is capable of generating new datasets of summaries, catering to different truncation lengths. This ensures a level playing field for all models under scrutiny.\n",
    "\n",
    "### Models for Comparison\n",
    "\n",
    "The models included in this analysis are:\n",
    "\n",
    "- **GPT-4:** An advanced language model known for its versatility and depth in understanding context.\n",
    "- **BART:** A transformer-based model designed for sequence-to-sequence tasks, excelling in summarization.\n",
    "- **Pegasus:** Specifically fine-tuned for abstractive text summarization, known for generating more coherent summaries.\n",
    "- **Text Summarization Model:** A general model for summarization tasks.\n",
    "- **LedLargeBookSummarization:** A model tailored for summarizing longer texts, such as books.\n",
    "\n",
    "### Complications Encountered\n",
    "\n",
    "- **Data Preparation:** Preparing the datasets for each model was a challenge, as each model had different input requirements and sensitivities. This\n",
    "\n",
    "- **Interpreting Results:** Deciphering the subtleties in the summaries generated by each model and comparing them objectively proved to be a complex task, given the subjective nature of summarization quality.\n",
    "\n",
    "- **Consistency in Evaluation:** Establishing a consistent and fair framework for evaluating the performance of each model was challenging, particularly in balancing quantitative metrics with qualitative assessments.\n",
    "\n",
    "### Experiment Design\n",
    "\n",
    "The experiment will be conducted as follows:\n",
    "\n",
    "1. **Input Preparation:** Identical input content will be fed into each language model. This ensures fairness and consistency in the comparison.\n",
    "2. **Truncation Variations:** The program will generate datasets for inputs with lengths of 500, 1000, and 1500 characters. This is to observe how the models perform under different input length constraints.\n",
    "3. **Quality Assessment:** The quality of the generated summaries will be evaluated. This will involve assessing coherence, relevancy, and conciseness.\n",
    "4. **Comparative Analysis:** Observations will be made on how the summaries vary across different models and input lengths.\n",
    "\n",
    "## Preliminary Notes Template\n",
    "\n",
    "For recording observations, the following template will be used:\n",
    "\n",
    "\n",
    "### Initial Notes on Models:\n",
    "\n",
    "#### GPT-4:\n",
    "- \n",
    "\n",
    "#### BART:\n",
    "- \n",
    "\n",
    "#### Pegasus:\n",
    "- \n",
    "\n",
    "#### Text Summarization:\n",
    "- \n",
    "\n",
    "#### LedLargeBookSummarization:\n",
    "- \n",
    "\n",
    "### Changes Observed:\n",
    "\n",
    "#### Between 500 and 1000 Characters:\n",
    "- \n",
    "\n",
    "#### Between 1000 and 1500 Characters:\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3.12' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('./TranscriptDataset.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify its structure\n",
    "display(df.head())\n",
    "\n",
    "##Helper functions for outputting summary data for comparison.\n",
    "def display_descriptions_and_summaries(df, row_index, num_lines, truncation_length):\n",
    "    \"\"\"\n",
    "    Display brief description and user specified number of lines from each summary column \n",
    "    for a given row, with text formatted according to the truncation length.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    row_index (int): The index of the row to display.\n",
    "    num_lines (int): The number of lines to display from each summary.\n",
    "    truncation_length (int): The number of characters after which to insert a newline in the summary text.\n",
    "    \"\"\"\n",
    "    # Select the specified row\n",
    "    selected_row = df.loc[row_index]\n",
    "\n",
    "    # Display the brief description\n",
    "    brief_description = selected_row.get('BriefDescription', 'No description provided')\n",
    "    print(f\"========\\nBrief Description (Row {row_index}):\\n{brief_description}\\n========\\n\")\n",
    "\n",
    "    # Display the specified number of lines from each summary column\n",
    "    for column in ['GPT-4 Summary', 'Gpt-3.5 Turbo Summary', 'LLama 2 70b Summary', 'Bard Summary']:\n",
    "        print(f\"Top {num_lines} lines of '{column}' (Row {row_index}):\")\n",
    "        \n",
    "        # Get the formatted text\n",
    "        formatted_text = format_summary_text(str(selected_row[column]), truncation_length)\n",
    "\n",
    "        # Print the specified number of lines\n",
    "        print('\\n'.join(formatted_text.split('\\n')[:num_lines]))\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "\n",
    "def format_summary_text(text, truncation_length):\n",
    "    \"\"\"\n",
    "    Formats the summary text by inserting a newline character after a specified number of characters.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be formatted.\n",
    "    truncation_length (int): The number of characters after which to insert a newline.\n",
    "\n",
    "    Returns:\n",
    "    str: The formatted text.\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Initialize variables to keep track of the current line length and the formatted text\n",
    "    current_line_length = 0\n",
    "    formatted_text = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        # Add the word to the current line\n",
    "        formatted_text += word + \" \"\n",
    "        current_line_length += len(word) + 1\n",
    "\n",
    "        # If the current line reaches or exceeds the truncation length, add a newline\n",
    "        if current_line_length >= truncation_length:\n",
    "            formatted_text += \"\\n\"\n",
    "            current_line_length = 0\n",
    "\n",
    "    return formatted_text\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "# Assuming the 'BriefDescription' column has been added to the DataFrame\n",
    "display_descriptions_and_summaries(df, row_index=0, num_lines=10, truncation_length=80)  # Adjust the arguments as needed\n",
    "\n",
    "\n",
    "print(\"\\n\\n Here is the analysis of the next video in dataset ...\\n\\n\")\n",
    "\n",
    "display_descriptions_and_summaries(df, row_index=1, num_lines=10, truncation_length=80)  # Adjust the arguments as needed\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
